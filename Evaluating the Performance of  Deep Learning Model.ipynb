{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jason Brownlee Book chapter 8 Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIeQeBJMobCxbDF2LEzM2W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AVI18794/DLND/blob/master/Evaluating%20the%20Performance%20of%20%20Deep%20Learning%20Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DeX9YspvpUK"
      },
      "source": [
        "# Evaluating the performance of Deep Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkXqVh9J3Oqj"
      },
      "source": [
        "## 1) Automatic Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us5ErYtPnVGf"
      },
      "source": [
        "#MLP with Automatic Validation Set\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "seed= 7\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs7jQqxRo9GM"
      },
      "source": [
        "import pandas as pd\n",
        "#Load the pima indian diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv\"\n",
        "dataset = pd.read_csv(url)\n",
        "#Split the data into input (X) and output (Y) variables\n",
        "X= dataset.iloc[:,:-1].values\n",
        "y = dataset.iloc[:,-1].values"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "mpmI2ecHp_Vm",
        "outputId": "40f244f4-a160-4fbf-aed4-aedcb1278533"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101</td>\n",
              "      <td>76</td>\n",
              "      <td>48</td>\n",
              "      <td>180</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122</td>\n",
              "      <td>70</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>112</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>70</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Pregnancies  Glucose  ...  Age  Outcome\n",
              "0              6      148  ...   50        1\n",
              "1              1       85  ...   31        0\n",
              "2              8      183  ...   32        1\n",
              "3              1       89  ...   21        0\n",
              "4              0      137  ...   33        1\n",
              "..           ...      ...  ...  ...      ...\n",
              "763           10      101  ...   63        0\n",
              "764            2      122  ...   27        0\n",
              "765            5      121  ...   30        0\n",
              "766            1      126  ...   47        1\n",
              "767            1       93  ...   23        0\n",
              "\n",
              "[768 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcFSI_e6qVBL",
        "outputId": "c3c75f9e-adfa-4758-8e6a-cf8df76b1af8"
      },
      "source": [
        "dataset.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4GKY4Y0pZwL",
        "outputId": "9a5185a9-73fc-4fb7-e85e-f7bb3e9f682c"
      },
      "source": [
        "X"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  6.   , 148.   ,  72.   , ...,  33.6  ,   0.627,  50.   ],\n",
              "       [  1.   ,  85.   ,  66.   , ...,  26.6  ,   0.351,  31.   ],\n",
              "       [  8.   , 183.   ,  64.   , ...,  23.3  ,   0.672,  32.   ],\n",
              "       ...,\n",
              "       [  5.   , 121.   ,  72.   , ...,  26.2  ,   0.245,  30.   ],\n",
              "       [  1.   , 126.   ,  60.   , ...,  30.1  ,   0.349,  47.   ],\n",
              "       [  1.   ,  93.   ,  70.   , ...,  30.4  ,   0.315,  23.   ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbEE3ZsApaHN",
        "outputId": "a72439e9-4f3c-4e00-8f2b-e2287a546d8a"
      },
      "source": [
        "y"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
              "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
              "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWVpJZTrqD_Y"
      },
      "source": [
        "#Define the neural network model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12,input_dim=8,kernel_initializer='uniform',activation='relu'))\n",
        "model.add(Dense(8,kernel_initializer='uniform',activation='relu'))\n",
        "model.add(Dense(1,kernel_initializer='uniform',activation='sigmoid'))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KSDbld9sEDP"
      },
      "source": [
        "#Compile the model\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEUT75FItF2Q",
        "outputId": "f0da0e27-091d-4036-9796-ee868c6bed5f"
      },
      "source": [
        "#Fit the model \n",
        "model.fit(X,y,epochs=150,verbose=1,batch_size=10,validation_split=0.33)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "52/52 [==============================] - 1s 9ms/step - loss: 0.6869 - accuracy: 0.6357 - val_loss: 0.6614 - val_accuracy: 0.6732\n",
            "Epoch 2/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6694 - accuracy: 0.6412 - val_loss: 0.6547 - val_accuracy: 0.6732\n",
            "Epoch 3/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.6452 - val_loss: 0.6495 - val_accuracy: 0.6811\n",
            "Epoch 4/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6434 - accuracy: 0.6477 - val_loss: 0.6420 - val_accuracy: 0.6811\n",
            "Epoch 5/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6407 - accuracy: 0.6489 - val_loss: 0.6333 - val_accuracy: 0.6850\n",
            "Epoch 6/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6291 - accuracy: 0.6528 - val_loss: 0.6356 - val_accuracy: 0.6575\n",
            "Epoch 7/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6508 - accuracy: 0.6391 - val_loss: 0.6189 - val_accuracy: 0.6929\n",
            "Epoch 8/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.6713 - val_loss: 0.6107 - val_accuracy: 0.6929\n",
            "Epoch 9/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6123 - accuracy: 0.6668 - val_loss: 0.6025 - val_accuracy: 0.7008\n",
            "Epoch 10/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6119 - accuracy: 0.6878 - val_loss: 0.6269 - val_accuracy: 0.6457\n",
            "Epoch 11/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6271 - accuracy: 0.6791 - val_loss: 0.5931 - val_accuracy: 0.7205\n",
            "Epoch 12/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5917 - accuracy: 0.7050 - val_loss: 0.5874 - val_accuracy: 0.6969\n",
            "Epoch 13/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6044 - accuracy: 0.6786 - val_loss: 0.5879 - val_accuracy: 0.6732\n",
            "Epoch 14/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6465 - accuracy: 0.6552 - val_loss: 0.5851 - val_accuracy: 0.6850\n",
            "Epoch 15/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6274 - accuracy: 0.6552 - val_loss: 0.5817 - val_accuracy: 0.6811\n",
            "Epoch 16/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6189 - accuracy: 0.6651 - val_loss: 0.5745 - val_accuracy: 0.7008\n",
            "Epoch 17/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6082 - accuracy: 0.6793 - val_loss: 0.5819 - val_accuracy: 0.6929\n",
            "Epoch 18/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5902 - accuracy: 0.7129 - val_loss: 0.5690 - val_accuracy: 0.7126\n",
            "Epoch 19/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5918 - accuracy: 0.7359 - val_loss: 0.5804 - val_accuracy: 0.6929\n",
            "Epoch 20/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5938 - accuracy: 0.6832 - val_loss: 0.5687 - val_accuracy: 0.7047\n",
            "Epoch 21/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6209 - accuracy: 0.6879 - val_loss: 0.5653 - val_accuracy: 0.6850\n",
            "Epoch 22/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6141 - accuracy: 0.6697 - val_loss: 0.5685 - val_accuracy: 0.6890\n",
            "Epoch 23/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5853 - accuracy: 0.7191 - val_loss: 0.5645 - val_accuracy: 0.7047\n",
            "Epoch 24/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6000 - accuracy: 0.6930 - val_loss: 0.5742 - val_accuracy: 0.7047\n",
            "Epoch 25/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6105 - accuracy: 0.6860 - val_loss: 0.5767 - val_accuracy: 0.7047\n",
            "Epoch 26/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6109 - accuracy: 0.6710 - val_loss: 0.5627 - val_accuracy: 0.7047\n",
            "Epoch 27/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5765 - accuracy: 0.7174 - val_loss: 0.5677 - val_accuracy: 0.6969\n",
            "Epoch 28/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5979 - accuracy: 0.7197 - val_loss: 0.5621 - val_accuracy: 0.6969\n",
            "Epoch 29/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5941 - accuracy: 0.7023 - val_loss: 0.5603 - val_accuracy: 0.7047\n",
            "Epoch 30/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5841 - accuracy: 0.6980 - val_loss: 0.5587 - val_accuracy: 0.7126\n",
            "Epoch 31/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5867 - accuracy: 0.7128 - val_loss: 0.5655 - val_accuracy: 0.6890\n",
            "Epoch 32/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5945 - accuracy: 0.7045 - val_loss: 0.5579 - val_accuracy: 0.7205\n",
            "Epoch 33/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5886 - accuracy: 0.6679 - val_loss: 0.5576 - val_accuracy: 0.7087\n",
            "Epoch 34/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5716 - accuracy: 0.7112 - val_loss: 0.5738 - val_accuracy: 0.7244\n",
            "Epoch 35/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5984 - accuracy: 0.6603 - val_loss: 0.5550 - val_accuracy: 0.7244\n",
            "Epoch 36/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5606 - accuracy: 0.7235 - val_loss: 0.5589 - val_accuracy: 0.6890\n",
            "Epoch 37/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5835 - accuracy: 0.6917 - val_loss: 0.5699 - val_accuracy: 0.6969\n",
            "Epoch 38/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5771 - accuracy: 0.7011 - val_loss: 0.5640 - val_accuracy: 0.6929\n",
            "Epoch 39/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5572 - accuracy: 0.7125 - val_loss: 0.5603 - val_accuracy: 0.7165\n",
            "Epoch 40/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5760 - accuracy: 0.7419 - val_loss: 0.5730 - val_accuracy: 0.7205\n",
            "Epoch 41/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5741 - accuracy: 0.7105 - val_loss: 0.5566 - val_accuracy: 0.7047\n",
            "Epoch 42/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5740 - accuracy: 0.7048 - val_loss: 0.5528 - val_accuracy: 0.7283\n",
            "Epoch 43/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5654 - accuracy: 0.6935 - val_loss: 0.5519 - val_accuracy: 0.7126\n",
            "Epoch 44/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5502 - accuracy: 0.7361 - val_loss: 0.5515 - val_accuracy: 0.7087\n",
            "Epoch 45/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5608 - accuracy: 0.7271 - val_loss: 0.5529 - val_accuracy: 0.7323\n",
            "Epoch 46/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5779 - accuracy: 0.7187 - val_loss: 0.5510 - val_accuracy: 0.7205\n",
            "Epoch 47/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5533 - accuracy: 0.6910 - val_loss: 0.5491 - val_accuracy: 0.7244\n",
            "Epoch 48/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5542 - accuracy: 0.7070 - val_loss: 0.5692 - val_accuracy: 0.6929\n",
            "Epoch 49/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6027 - accuracy: 0.6868 - val_loss: 0.5516 - val_accuracy: 0.7165\n",
            "Epoch 50/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5876 - accuracy: 0.7277 - val_loss: 0.5547 - val_accuracy: 0.7362\n",
            "Epoch 51/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5709 - accuracy: 0.7307 - val_loss: 0.5666 - val_accuracy: 0.7087\n",
            "Epoch 52/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5607 - accuracy: 0.7240 - val_loss: 0.5488 - val_accuracy: 0.7244\n",
            "Epoch 53/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5632 - accuracy: 0.7212 - val_loss: 0.5469 - val_accuracy: 0.7205\n",
            "Epoch 54/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5548 - accuracy: 0.7436 - val_loss: 0.5490 - val_accuracy: 0.7323\n",
            "Epoch 55/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7177 - val_loss: 0.5511 - val_accuracy: 0.7126\n",
            "Epoch 56/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5618 - accuracy: 0.7293 - val_loss: 0.5485 - val_accuracy: 0.7087\n",
            "Epoch 57/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5610 - accuracy: 0.7185 - val_loss: 0.5740 - val_accuracy: 0.7047\n",
            "Epoch 58/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5830 - accuracy: 0.6886 - val_loss: 0.5438 - val_accuracy: 0.7244\n",
            "Epoch 59/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7920 - val_loss: 0.5502 - val_accuracy: 0.7126\n",
            "Epoch 60/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5812 - accuracy: 0.7036 - val_loss: 0.5434 - val_accuracy: 0.7205\n",
            "Epoch 61/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.6893 - val_loss: 0.5431 - val_accuracy: 0.7362\n",
            "Epoch 62/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5733 - accuracy: 0.7075 - val_loss: 0.5537 - val_accuracy: 0.6929\n",
            "Epoch 63/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5704 - accuracy: 0.7244 - val_loss: 0.5513 - val_accuracy: 0.7441\n",
            "Epoch 64/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5536 - accuracy: 0.7200 - val_loss: 0.5503 - val_accuracy: 0.7323\n",
            "Epoch 65/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5591 - accuracy: 0.7252 - val_loss: 0.5416 - val_accuracy: 0.7323\n",
            "Epoch 66/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5659 - accuracy: 0.7225 - val_loss: 0.5477 - val_accuracy: 0.7441\n",
            "Epoch 67/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5662 - accuracy: 0.7014 - val_loss: 0.5457 - val_accuracy: 0.7126\n",
            "Epoch 68/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5851 - accuracy: 0.7068 - val_loss: 0.5377 - val_accuracy: 0.7362\n",
            "Epoch 69/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5684 - accuracy: 0.7167 - val_loss: 0.5404 - val_accuracy: 0.7323\n",
            "Epoch 70/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5356 - accuracy: 0.7151 - val_loss: 0.5413 - val_accuracy: 0.7323\n",
            "Epoch 71/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5495 - accuracy: 0.7243 - val_loss: 0.5400 - val_accuracy: 0.7402\n",
            "Epoch 72/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5582 - accuracy: 0.6983 - val_loss: 0.5362 - val_accuracy: 0.7480\n",
            "Epoch 73/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5750 - accuracy: 0.6995 - val_loss: 0.5359 - val_accuracy: 0.7520\n",
            "Epoch 74/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5233 - accuracy: 0.7290 - val_loss: 0.5410 - val_accuracy: 0.7441\n",
            "Epoch 75/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7575 - val_loss: 0.5351 - val_accuracy: 0.7520\n",
            "Epoch 76/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.7453 - val_loss: 0.5359 - val_accuracy: 0.7520\n",
            "Epoch 77/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.7752 - val_loss: 0.5419 - val_accuracy: 0.7480\n",
            "Epoch 78/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5414 - accuracy: 0.7541 - val_loss: 0.5603 - val_accuracy: 0.7323\n",
            "Epoch 79/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5494 - accuracy: 0.7244 - val_loss: 0.5321 - val_accuracy: 0.7638\n",
            "Epoch 80/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5687 - accuracy: 0.7059 - val_loss: 0.5295 - val_accuracy: 0.7480\n",
            "Epoch 81/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5274 - accuracy: 0.7441 - val_loss: 0.5292 - val_accuracy: 0.7638\n",
            "Epoch 82/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5139 - accuracy: 0.7529 - val_loss: 0.5297 - val_accuracy: 0.7677\n",
            "Epoch 83/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5289 - accuracy: 0.7379 - val_loss: 0.5906 - val_accuracy: 0.6929\n",
            "Epoch 84/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5636 - accuracy: 0.7390 - val_loss: 0.5299 - val_accuracy: 0.7638\n",
            "Epoch 85/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5548 - accuracy: 0.7300 - val_loss: 0.5320 - val_accuracy: 0.7244\n",
            "Epoch 86/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5224 - accuracy: 0.7556 - val_loss: 0.5290 - val_accuracy: 0.7677\n",
            "Epoch 87/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.7500 - val_loss: 0.5451 - val_accuracy: 0.7480\n",
            "Epoch 88/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5607 - accuracy: 0.7199 - val_loss: 0.5293 - val_accuracy: 0.7283\n",
            "Epoch 89/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.7749 - val_loss: 0.5330 - val_accuracy: 0.7520\n",
            "Epoch 90/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5594 - accuracy: 0.7070 - val_loss: 0.5598 - val_accuracy: 0.7205\n",
            "Epoch 91/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5411 - accuracy: 0.7214 - val_loss: 0.5410 - val_accuracy: 0.7087\n",
            "Epoch 92/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.7309 - val_loss: 0.5273 - val_accuracy: 0.7283\n",
            "Epoch 93/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5263 - accuracy: 0.7260 - val_loss: 0.5321 - val_accuracy: 0.7402\n",
            "Epoch 94/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5391 - accuracy: 0.7485 - val_loss: 0.5312 - val_accuracy: 0.7598\n",
            "Epoch 95/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5442 - accuracy: 0.7462 - val_loss: 0.5184 - val_accuracy: 0.7677\n",
            "Epoch 96/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5587 - accuracy: 0.7267 - val_loss: 0.5268 - val_accuracy: 0.7362\n",
            "Epoch 97/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5326 - accuracy: 0.7528 - val_loss: 0.5145 - val_accuracy: 0.7598\n",
            "Epoch 98/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5154 - accuracy: 0.7572 - val_loss: 0.5381 - val_accuracy: 0.7402\n",
            "Epoch 99/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5153 - accuracy: 0.7365 - val_loss: 0.5216 - val_accuracy: 0.7520\n",
            "Epoch 100/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7689 - val_loss: 0.5182 - val_accuracy: 0.7835\n",
            "Epoch 101/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5154 - accuracy: 0.7480 - val_loss: 0.5239 - val_accuracy: 0.7559\n",
            "Epoch 102/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5496 - accuracy: 0.7250 - val_loss: 0.5361 - val_accuracy: 0.7480\n",
            "Epoch 103/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5318 - accuracy: 0.7483 - val_loss: 0.5591 - val_accuracy: 0.7126\n",
            "Epoch 104/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5400 - accuracy: 0.7316 - val_loss: 0.5152 - val_accuracy: 0.7638\n",
            "Epoch 105/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5175 - accuracy: 0.7621 - val_loss: 0.5118 - val_accuracy: 0.7756\n",
            "Epoch 106/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5545 - accuracy: 0.7072 - val_loss: 0.5134 - val_accuracy: 0.7480\n",
            "Epoch 107/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7461 - val_loss: 0.5100 - val_accuracy: 0.7835\n",
            "Epoch 108/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7485 - val_loss: 0.5363 - val_accuracy: 0.7205\n",
            "Epoch 109/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5545 - accuracy: 0.7430 - val_loss: 0.5181 - val_accuracy: 0.7717\n",
            "Epoch 110/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5264 - accuracy: 0.7422 - val_loss: 0.5099 - val_accuracy: 0.7992\n",
            "Epoch 111/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5015 - accuracy: 0.7796 - val_loss: 0.5090 - val_accuracy: 0.7953\n",
            "Epoch 112/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5186 - accuracy: 0.7403 - val_loss: 0.5213 - val_accuracy: 0.7441\n",
            "Epoch 113/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5456 - accuracy: 0.7218 - val_loss: 0.5133 - val_accuracy: 0.7874\n",
            "Epoch 114/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5175 - accuracy: 0.7536 - val_loss: 0.5221 - val_accuracy: 0.7598\n",
            "Epoch 115/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5216 - accuracy: 0.7269 - val_loss: 0.5131 - val_accuracy: 0.7638\n",
            "Epoch 116/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4978 - accuracy: 0.7633 - val_loss: 0.5077 - val_accuracy: 0.7874\n",
            "Epoch 117/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7434 - val_loss: 0.5314 - val_accuracy: 0.7480\n",
            "Epoch 118/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5286 - accuracy: 0.7402 - val_loss: 0.5078 - val_accuracy: 0.7756\n",
            "Epoch 119/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7615 - val_loss: 0.5102 - val_accuracy: 0.7559\n",
            "Epoch 120/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7389 - val_loss: 0.5229 - val_accuracy: 0.7638\n",
            "Epoch 121/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5016 - accuracy: 0.7719 - val_loss: 0.5141 - val_accuracy: 0.7913\n",
            "Epoch 122/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5535 - accuracy: 0.7103 - val_loss: 0.5167 - val_accuracy: 0.7362\n",
            "Epoch 123/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.7568 - val_loss: 0.5053 - val_accuracy: 0.7717\n",
            "Epoch 124/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7461 - val_loss: 0.5162 - val_accuracy: 0.7402\n",
            "Epoch 125/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5052 - accuracy: 0.7664 - val_loss: 0.5001 - val_accuracy: 0.7992\n",
            "Epoch 126/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5325 - accuracy: 0.7203 - val_loss: 0.5054 - val_accuracy: 0.7953\n",
            "Epoch 127/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7730 - val_loss: 0.5251 - val_accuracy: 0.7283\n",
            "Epoch 128/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.7087 - val_loss: 0.5123 - val_accuracy: 0.7441\n",
            "Epoch 129/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5302 - accuracy: 0.7372 - val_loss: 0.5058 - val_accuracy: 0.7992\n",
            "Epoch 130/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5672 - accuracy: 0.6956 - val_loss: 0.4982 - val_accuracy: 0.7835\n",
            "Epoch 131/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5072 - accuracy: 0.7477 - val_loss: 0.5018 - val_accuracy: 0.7835\n",
            "Epoch 132/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7457 - val_loss: 0.4974 - val_accuracy: 0.7992\n",
            "Epoch 133/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7588 - val_loss: 0.5019 - val_accuracy: 0.7992\n",
            "Epoch 134/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4939 - accuracy: 0.7759 - val_loss: 0.5042 - val_accuracy: 0.7520\n",
            "Epoch 135/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4970 - accuracy: 0.7466 - val_loss: 0.5076 - val_accuracy: 0.7874\n",
            "Epoch 136/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7842 - val_loss: 0.5142 - val_accuracy: 0.7874\n",
            "Epoch 137/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.7664 - val_loss: 0.4958 - val_accuracy: 0.7953\n",
            "Epoch 138/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.7588 - val_loss: 0.5070 - val_accuracy: 0.7559\n",
            "Epoch 139/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5509 - accuracy: 0.6964 - val_loss: 0.4936 - val_accuracy: 0.7874\n",
            "Epoch 140/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7689 - val_loss: 0.5081 - val_accuracy: 0.7913\n",
            "Epoch 141/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5013 - accuracy: 0.7512 - val_loss: 0.5085 - val_accuracy: 0.7953\n",
            "Epoch 142/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.7697 - val_loss: 0.5132 - val_accuracy: 0.7441\n",
            "Epoch 143/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.7724 - val_loss: 0.4975 - val_accuracy: 0.7874\n",
            "Epoch 144/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4731 - accuracy: 0.7766 - val_loss: 0.4911 - val_accuracy: 0.7835\n",
            "Epoch 145/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7653 - val_loss: 0.4944 - val_accuracy: 0.7835\n",
            "Epoch 146/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4673 - accuracy: 0.7719 - val_loss: 0.5197 - val_accuracy: 0.7717\n",
            "Epoch 147/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.7569 - val_loss: 0.4901 - val_accuracy: 0.7874\n",
            "Epoch 148/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4677 - accuracy: 0.7827 - val_loss: 0.5194 - val_accuracy: 0.7323\n",
            "Epoch 149/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5636 - accuracy: 0.7400 - val_loss: 0.4872 - val_accuracy: 0.7835\n",
            "Epoch 150/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.7514 - val_loss: 0.4904 - val_accuracy: 0.7835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe82b0c8410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o8jp2zptc1u",
        "outputId": "a0f23d7c-0b21-4ebb-ac4f-d7fc7bb249eb"
      },
      "source": [
        "#Evaluate the model\n",
        "#Evaluate the model\n",
        "scores = model.evaluate(X,y)\n",
        "print(\"%s: %.2f%%\"%(model.metrics_names[1],scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 1ms/step - loss: 0.4742 - accuracy: 0.7669\n",
            "accuracy: 76.69%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZsoGN2qwbis"
      },
      "source": [
        "## 2)  Manual Verification Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSvSoTM7t7AR"
      },
      "source": [
        "#MLP with manual validation set\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "np.random.seed(7)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbDtExqaw9eq",
        "outputId": "c15d63a4-60f8-4f14-d32e-16ef9f9791ef"
      },
      "source": [
        "dataset = pd.read_csv(url)\n",
        "X = dataset.iloc[:,:-1].values\n",
        "y = dataset.iloc[:,-1].values\n",
        "#Split the dataset into 67% for train and 33% for testing\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=seed)\n",
        "#Create the model\n",
        "model = Sequential()\n",
        "model.add(Dense(12,input_dim=8,kernel_initializer='uniform',activation='relu'))\n",
        "model.add(Dense(8,kernel_initializer='uniform',activation='relu'))\n",
        "model.add(Dense(1,kernel_initializer='uniform',activation='sigmoid'))\n",
        "#Compile the model\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "#Fit the model\n",
        "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=150,batch_size=10)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "52/52 [==============================] - 1s 5ms/step - loss: 0.6897 - accuracy: 0.5838 - val_loss: 0.6760 - val_accuracy: 0.6378\n",
            "Epoch 2/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6688 - accuracy: 0.6665 - val_loss: 0.6686 - val_accuracy: 0.6378\n",
            "Epoch 3/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6565 - accuracy: 0.6731 - val_loss: 0.6632 - val_accuracy: 0.6378\n",
            "Epoch 4/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6587 - accuracy: 0.6441 - val_loss: 0.6574 - val_accuracy: 0.6378\n",
            "Epoch 5/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6546 - accuracy: 0.6500 - val_loss: 0.6527 - val_accuracy: 0.6457\n",
            "Epoch 6/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6513 - accuracy: 0.6586 - val_loss: 0.6479 - val_accuracy: 0.6457\n",
            "Epoch 7/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6410 - accuracy: 0.6585 - val_loss: 0.6398 - val_accuracy: 0.6732\n",
            "Epoch 8/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6505 - accuracy: 0.6614 - val_loss: 0.6355 - val_accuracy: 0.6339\n",
            "Epoch 9/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6085 - accuracy: 0.7062 - val_loss: 0.6266 - val_accuracy: 0.6732\n",
            "Epoch 10/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6344 - accuracy: 0.6411 - val_loss: 0.6176 - val_accuracy: 0.6732\n",
            "Epoch 11/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6129 - accuracy: 0.6677 - val_loss: 0.6100 - val_accuracy: 0.6890\n",
            "Epoch 12/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6044 - accuracy: 0.6814 - val_loss: 0.6119 - val_accuracy: 0.6811\n",
            "Epoch 13/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6155 - accuracy: 0.6791 - val_loss: 0.6143 - val_accuracy: 0.6693\n",
            "Epoch 14/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6062 - accuracy: 0.6990 - val_loss: 0.5961 - val_accuracy: 0.6890\n",
            "Epoch 15/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6037 - accuracy: 0.7024 - val_loss: 0.5969 - val_accuracy: 0.6811\n",
            "Epoch 16/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6157 - accuracy: 0.6842 - val_loss: 0.5912 - val_accuracy: 0.7126\n",
            "Epoch 17/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5633 - accuracy: 0.7257 - val_loss: 0.5905 - val_accuracy: 0.7008\n",
            "Epoch 18/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6032 - accuracy: 0.7012 - val_loss: 0.5893 - val_accuracy: 0.7008\n",
            "Epoch 19/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5974 - accuracy: 0.6745 - val_loss: 0.5864 - val_accuracy: 0.7008\n",
            "Epoch 20/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5997 - accuracy: 0.7195 - val_loss: 0.5993 - val_accuracy: 0.6772\n",
            "Epoch 21/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5847 - accuracy: 0.7114 - val_loss: 0.5868 - val_accuracy: 0.7008\n",
            "Epoch 22/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.7061 - val_loss: 0.5847 - val_accuracy: 0.6969\n",
            "Epoch 23/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5765 - accuracy: 0.6992 - val_loss: 0.5886 - val_accuracy: 0.7008\n",
            "Epoch 24/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5763 - accuracy: 0.6971 - val_loss: 0.5859 - val_accuracy: 0.6929\n",
            "Epoch 25/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5638 - accuracy: 0.7040 - val_loss: 0.5950 - val_accuracy: 0.6929\n",
            "Epoch 26/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5667 - accuracy: 0.7194 - val_loss: 0.5874 - val_accuracy: 0.6929\n",
            "Epoch 27/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5545 - accuracy: 0.7397 - val_loss: 0.5823 - val_accuracy: 0.6929\n",
            "Epoch 28/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5556 - accuracy: 0.7085 - val_loss: 0.5857 - val_accuracy: 0.6929\n",
            "Epoch 29/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5732 - accuracy: 0.7001 - val_loss: 0.5898 - val_accuracy: 0.6850\n",
            "Epoch 30/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5510 - accuracy: 0.7183 - val_loss: 0.5807 - val_accuracy: 0.6890\n",
            "Epoch 31/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5380 - accuracy: 0.7666 - val_loss: 0.5830 - val_accuracy: 0.6969\n",
            "Epoch 32/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5646 - accuracy: 0.7067 - val_loss: 0.5802 - val_accuracy: 0.7008\n",
            "Epoch 33/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5712 - accuracy: 0.6961 - val_loss: 0.5797 - val_accuracy: 0.7008\n",
            "Epoch 34/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5455 - accuracy: 0.7380 - val_loss: 0.5882 - val_accuracy: 0.6890\n",
            "Epoch 35/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6206 - accuracy: 0.6848 - val_loss: 0.5853 - val_accuracy: 0.6929\n",
            "Epoch 36/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5680 - accuracy: 0.7211 - val_loss: 0.5841 - val_accuracy: 0.6890\n",
            "Epoch 37/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5833 - accuracy: 0.6771 - val_loss: 0.5761 - val_accuracy: 0.7008\n",
            "Epoch 38/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5481 - accuracy: 0.7019 - val_loss: 0.5731 - val_accuracy: 0.7008\n",
            "Epoch 39/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7278 - val_loss: 0.5762 - val_accuracy: 0.6969\n",
            "Epoch 40/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5844 - accuracy: 0.7049 - val_loss: 0.5765 - val_accuracy: 0.7087\n",
            "Epoch 41/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5808 - accuracy: 0.7063 - val_loss: 0.5755 - val_accuracy: 0.6890\n",
            "Epoch 42/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5522 - accuracy: 0.7430 - val_loss: 0.5779 - val_accuracy: 0.6850\n",
            "Epoch 43/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5663 - accuracy: 0.6922 - val_loss: 0.5736 - val_accuracy: 0.6890\n",
            "Epoch 44/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5501 - accuracy: 0.7335 - val_loss: 0.5745 - val_accuracy: 0.7008\n",
            "Epoch 45/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5621 - accuracy: 0.7252 - val_loss: 0.5772 - val_accuracy: 0.7008\n",
            "Epoch 46/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5692 - accuracy: 0.7133 - val_loss: 0.5777 - val_accuracy: 0.6969\n",
            "Epoch 47/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5601 - accuracy: 0.7521 - val_loss: 0.5718 - val_accuracy: 0.6969\n",
            "Epoch 48/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7209 - val_loss: 0.5699 - val_accuracy: 0.6969\n",
            "Epoch 49/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5370 - accuracy: 0.7396 - val_loss: 0.5784 - val_accuracy: 0.6969\n",
            "Epoch 50/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5441 - accuracy: 0.7286 - val_loss: 0.5704 - val_accuracy: 0.7008\n",
            "Epoch 51/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5812 - accuracy: 0.6847 - val_loss: 0.5714 - val_accuracy: 0.7008\n",
            "Epoch 52/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5465 - accuracy: 0.7338 - val_loss: 0.5739 - val_accuracy: 0.6850\n",
            "Epoch 53/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5374 - accuracy: 0.7238 - val_loss: 0.5639 - val_accuracy: 0.7008\n",
            "Epoch 54/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5379 - accuracy: 0.7046 - val_loss: 0.5731 - val_accuracy: 0.7047\n",
            "Epoch 55/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5366 - accuracy: 0.7367 - val_loss: 0.5776 - val_accuracy: 0.6929\n",
            "Epoch 56/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5495 - accuracy: 0.7275 - val_loss: 0.5655 - val_accuracy: 0.7126\n",
            "Epoch 57/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5579 - accuracy: 0.7308 - val_loss: 0.5730 - val_accuracy: 0.7008\n",
            "Epoch 58/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5390 - accuracy: 0.7408 - val_loss: 0.5716 - val_accuracy: 0.6969\n",
            "Epoch 59/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5471 - accuracy: 0.7432 - val_loss: 0.5642 - val_accuracy: 0.7126\n",
            "Epoch 60/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4971 - accuracy: 0.7421 - val_loss: 0.5804 - val_accuracy: 0.6732\n",
            "Epoch 61/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5516 - accuracy: 0.7179 - val_loss: 0.5711 - val_accuracy: 0.6929\n",
            "Epoch 62/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5421 - accuracy: 0.7141 - val_loss: 0.5628 - val_accuracy: 0.7165\n",
            "Epoch 63/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.7289 - val_loss: 0.5617 - val_accuracy: 0.7165\n",
            "Epoch 64/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.7739 - val_loss: 0.5601 - val_accuracy: 0.7244\n",
            "Epoch 65/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5191 - accuracy: 0.7455 - val_loss: 0.5628 - val_accuracy: 0.7165\n",
            "Epoch 66/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5694 - accuracy: 0.7244 - val_loss: 0.5666 - val_accuracy: 0.7126\n",
            "Epoch 67/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.7272 - val_loss: 0.5721 - val_accuracy: 0.7087\n",
            "Epoch 68/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7799 - val_loss: 0.5583 - val_accuracy: 0.6929\n",
            "Epoch 69/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5318 - accuracy: 0.7488 - val_loss: 0.5560 - val_accuracy: 0.6969\n",
            "Epoch 70/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7677 - val_loss: 0.5559 - val_accuracy: 0.7323\n",
            "Epoch 71/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7363 - val_loss: 0.5585 - val_accuracy: 0.7165\n",
            "Epoch 72/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.7282 - val_loss: 0.5619 - val_accuracy: 0.7087\n",
            "Epoch 73/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5383 - accuracy: 0.7353 - val_loss: 0.5557 - val_accuracy: 0.7126\n",
            "Epoch 74/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7416 - val_loss: 0.5542 - val_accuracy: 0.7126\n",
            "Epoch 75/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5259 - accuracy: 0.7474 - val_loss: 0.5872 - val_accuracy: 0.6850\n",
            "Epoch 76/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.7271 - val_loss: 0.5539 - val_accuracy: 0.7165\n",
            "Epoch 77/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.7533 - val_loss: 0.5907 - val_accuracy: 0.6890\n",
            "Epoch 78/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5296 - accuracy: 0.7373 - val_loss: 0.5583 - val_accuracy: 0.7244\n",
            "Epoch 79/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5164 - accuracy: 0.7499 - val_loss: 0.5621 - val_accuracy: 0.7205\n",
            "Epoch 80/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.7839 - val_loss: 0.5483 - val_accuracy: 0.7244\n",
            "Epoch 81/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7670 - val_loss: 0.5518 - val_accuracy: 0.7283\n",
            "Epoch 82/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5162 - accuracy: 0.7274 - val_loss: 0.5494 - val_accuracy: 0.7244\n",
            "Epoch 83/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5587 - accuracy: 0.7254 - val_loss: 0.5506 - val_accuracy: 0.7283\n",
            "Epoch 84/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7474 - val_loss: 0.5662 - val_accuracy: 0.6929\n",
            "Epoch 85/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5205 - accuracy: 0.7682 - val_loss: 0.5492 - val_accuracy: 0.7323\n",
            "Epoch 86/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5405 - accuracy: 0.7427 - val_loss: 0.5507 - val_accuracy: 0.7283\n",
            "Epoch 87/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7402 - val_loss: 0.5542 - val_accuracy: 0.7126\n",
            "Epoch 88/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4959 - accuracy: 0.7729 - val_loss: 0.5495 - val_accuracy: 0.7205\n",
            "Epoch 89/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5486 - accuracy: 0.7089 - val_loss: 0.5475 - val_accuracy: 0.7323\n",
            "Epoch 90/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5320 - accuracy: 0.7308 - val_loss: 0.5537 - val_accuracy: 0.7244\n",
            "Epoch 91/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5207 - accuracy: 0.7390 - val_loss: 0.5726 - val_accuracy: 0.7244\n",
            "Epoch 92/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7596 - val_loss: 0.5465 - val_accuracy: 0.7283\n",
            "Epoch 93/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4759 - accuracy: 0.7873 - val_loss: 0.5456 - val_accuracy: 0.7205\n",
            "Epoch 94/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5078 - accuracy: 0.7699 - val_loss: 0.5429 - val_accuracy: 0.7205\n",
            "Epoch 95/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.7372 - val_loss: 0.5410 - val_accuracy: 0.7205\n",
            "Epoch 96/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5423 - accuracy: 0.7285 - val_loss: 0.5411 - val_accuracy: 0.7323\n",
            "Epoch 97/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7807 - val_loss: 0.5514 - val_accuracy: 0.7283\n",
            "Epoch 98/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5020 - accuracy: 0.7647 - val_loss: 0.5419 - val_accuracy: 0.7165\n",
            "Epoch 99/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5159 - accuracy: 0.7382 - val_loss: 0.5461 - val_accuracy: 0.7244\n",
            "Epoch 100/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4564 - accuracy: 0.8018 - val_loss: 0.5431 - val_accuracy: 0.7323\n",
            "Epoch 101/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.7835 - val_loss: 0.5408 - val_accuracy: 0.7283\n",
            "Epoch 102/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7633 - val_loss: 0.5356 - val_accuracy: 0.7283\n",
            "Epoch 103/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7703 - val_loss: 0.5353 - val_accuracy: 0.7283\n",
            "Epoch 104/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7629 - val_loss: 0.5339 - val_accuracy: 0.7323\n",
            "Epoch 105/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5285 - accuracy: 0.7416 - val_loss: 0.5520 - val_accuracy: 0.7362\n",
            "Epoch 106/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7616 - val_loss: 0.5444 - val_accuracy: 0.7205\n",
            "Epoch 107/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4722 - accuracy: 0.7906 - val_loss: 0.5419 - val_accuracy: 0.7283\n",
            "Epoch 108/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7684 - val_loss: 0.5324 - val_accuracy: 0.7520\n",
            "Epoch 109/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4538 - accuracy: 0.7955 - val_loss: 0.5293 - val_accuracy: 0.7402\n",
            "Epoch 110/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.7679 - val_loss: 0.5552 - val_accuracy: 0.7205\n",
            "Epoch 111/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.7697 - val_loss: 0.5311 - val_accuracy: 0.7205\n",
            "Epoch 112/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4866 - accuracy: 0.7762 - val_loss: 0.5469 - val_accuracy: 0.7402\n",
            "Epoch 113/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4838 - accuracy: 0.7922 - val_loss: 0.5371 - val_accuracy: 0.7205\n",
            "Epoch 114/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7701 - val_loss: 0.5269 - val_accuracy: 0.7283\n",
            "Epoch 115/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.7728 - val_loss: 0.5281 - val_accuracy: 0.7480\n",
            "Epoch 116/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4848 - accuracy: 0.7795 - val_loss: 0.5382 - val_accuracy: 0.7165\n",
            "Epoch 117/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.8179 - val_loss: 0.5278 - val_accuracy: 0.7598\n",
            "Epoch 118/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4865 - accuracy: 0.7692 - val_loss: 0.5478 - val_accuracy: 0.7244\n",
            "Epoch 119/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4816 - accuracy: 0.7701 - val_loss: 0.5360 - val_accuracy: 0.7323\n",
            "Epoch 120/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.7672 - val_loss: 0.5280 - val_accuracy: 0.7441\n",
            "Epoch 121/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4953 - accuracy: 0.7712 - val_loss: 0.5301 - val_accuracy: 0.7283\n",
            "Epoch 122/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.8177 - val_loss: 0.5279 - val_accuracy: 0.7244\n",
            "Epoch 123/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.7789 - val_loss: 0.5212 - val_accuracy: 0.7362\n",
            "Epoch 124/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5009 - accuracy: 0.7562 - val_loss: 0.5185 - val_accuracy: 0.7402\n",
            "Epoch 125/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4459 - accuracy: 0.8096 - val_loss: 0.5221 - val_accuracy: 0.7559\n",
            "Epoch 126/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4947 - accuracy: 0.7568 - val_loss: 0.5265 - val_accuracy: 0.7362\n",
            "Epoch 127/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4751 - accuracy: 0.7809 - val_loss: 0.5199 - val_accuracy: 0.7480\n",
            "Epoch 128/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4650 - accuracy: 0.8008 - val_loss: 0.5370 - val_accuracy: 0.7323\n",
            "Epoch 129/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4603 - accuracy: 0.7926 - val_loss: 0.5186 - val_accuracy: 0.7559\n",
            "Epoch 130/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.7849 - val_loss: 0.5272 - val_accuracy: 0.7638\n",
            "Epoch 131/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.8221 - val_loss: 0.5202 - val_accuracy: 0.7402\n",
            "Epoch 132/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4632 - accuracy: 0.7758 - val_loss: 0.5206 - val_accuracy: 0.7520\n",
            "Epoch 133/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.7862 - val_loss: 0.5276 - val_accuracy: 0.7480\n",
            "Epoch 134/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4534 - accuracy: 0.7969 - val_loss: 0.5844 - val_accuracy: 0.7165\n",
            "Epoch 135/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7682 - val_loss: 0.5297 - val_accuracy: 0.7402\n",
            "Epoch 136/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.7971 - val_loss: 0.5245 - val_accuracy: 0.7362\n",
            "Epoch 137/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4561 - accuracy: 0.8036 - val_loss: 0.5305 - val_accuracy: 0.7402\n",
            "Epoch 138/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.7641 - val_loss: 0.5255 - val_accuracy: 0.7283\n",
            "Epoch 139/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4833 - accuracy: 0.7740 - val_loss: 0.5278 - val_accuracy: 0.7244\n",
            "Epoch 140/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4589 - accuracy: 0.7946 - val_loss: 0.5255 - val_accuracy: 0.7441\n",
            "Epoch 141/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4722 - accuracy: 0.7943 - val_loss: 0.5228 - val_accuracy: 0.7323\n",
            "Epoch 142/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.7704 - val_loss: 0.5259 - val_accuracy: 0.7402\n",
            "Epoch 143/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4659 - accuracy: 0.8035 - val_loss: 0.5519 - val_accuracy: 0.7402\n",
            "Epoch 144/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.7837 - val_loss: 0.5248 - val_accuracy: 0.7283\n",
            "Epoch 145/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.7770 - val_loss: 0.5262 - val_accuracy: 0.7165\n",
            "Epoch 146/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.7531 - val_loss: 0.5419 - val_accuracy: 0.7283\n",
            "Epoch 147/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.7600 - val_loss: 0.5134 - val_accuracy: 0.7520\n",
            "Epoch 148/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4338 - accuracy: 0.8149 - val_loss: 0.5398 - val_accuracy: 0.7402\n",
            "Epoch 149/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4670 - accuracy: 0.7793 - val_loss: 0.5426 - val_accuracy: 0.7244\n",
            "Epoch 150/150\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.8159 - val_loss: 0.5143 - val_accuracy: 0.7598\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe81f85b290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-SavdHhywMi"
      },
      "source": [
        "## 3) Manual k-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDfGzFL2ysXJ"
      },
      "source": [
        "#MLP for Pima Indians Diabetes Dataset with 10-fold cross validation\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(7)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X2LSo-w1AU9"
      },
      "source": [
        "dataset = pd.read_csv(url)\n",
        "X = dataset.iloc[:,:-1].values\n",
        "y = dataset.iloc[:,-1].values"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qenxXqsj1RFn",
        "outputId": "0a6b60d5-ef6d-4738-a62e-bad7d6e19d61"
      },
      "source": [
        "kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=7)\n",
        "cvscores =[]\n",
        "for train,test in kfold.split(X,y):\n",
        "  #Create the model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(12,input_dim=8,kernel_initializer='uniform',activation='relu'))\n",
        "  model.add(Dense(8,kernel_initializer='uniform',activation='relu'))\n",
        "  model.add(Dense(1,kernel_initializer='uniform',activation='sigmoid'))\n",
        "  #Compile the model\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  #Fit the model\n",
        "  model.fit(X[train],y[train],epochs=150,batch_size=10,verbose=0)\n",
        "  scores = model.evaluate(X[test],y[test],verbose=0)\n",
        "  print(\"%s : %.2f%%\"%(model.metrics_names[1],scores[1]*100))\n",
        "  cvscores.append(scores[1]*100)\n",
        "print(\"%.2f%%(+/-%.2f%%)\"%(np.mean(cvscores),np.std(cvscores))) "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy : 81.82%\n",
            "accuracy : 74.03%\n",
            "accuracy : 80.52%\n",
            "WARNING:tensorflow:5 out of the last 3910 calls to <function Model.make_test_function.<locals>.test_function at 0x7fe8198c9320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy : 77.92%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fe81974c200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy : 66.23%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fe81558b290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy : 66.23%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fe81447c8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy : 79.22%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fe813b6c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy : 76.62%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fe81218ddd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy : 76.32%\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fe81fc0ec20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "accuracy : 81.58%\n",
            "76.05%(+/-5.43%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M4oN6Na2rho"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}